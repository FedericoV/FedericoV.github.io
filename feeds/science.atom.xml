<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Federico's Blog</title><link href="http://federicov.github.io/" rel="alternate"></link><link href="http://federicov.github.io/feeds/science.atom.xml" rel="self"></link><id>http://federicov.github.io/</id><updated>2016-04-14T00:00:00+02:00</updated><entry><title>The peer review process of eLife</title><link href="http://federicov.github.io/peer-review-at-eLife.html" rel="alternate"></link><published>2016-04-14T00:00:00+02:00</published><updated>2016-04-14T00:00:00+02:00</updated><author><name>Federico Vaggi</name></author><id>tag:federicov.github.io,2016-04-14:peer-review-at-eLife.html</id><summary type="html">&lt;p&gt;Together with my colleagues Marco Giordan, Andy Collins, and Attila Csikasz-Nagy
we just published an analysis of the peer review process of eLife on
F1000 &lt;a href="http://f1000research.com/articles/5-683/v1"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was an interesting experience, and it was my first time leading a project from
the start all the way to publication (and now, post publication peer review, as
it should be).&lt;/p&gt;
&lt;p&gt;The idea to critically analyze peer review came after NIPS (one of the most
prestigious machine learning conferences) decided to do a very brave experiment
and publicly examine their own peer review process.  Briefly: for a subset of papers, NIPS
assigned two completely independent teams of reviewers to review.  After, they
examined the overlap in the agreement between the two sets of reviewers, and
the results were pretty bleak.  For a longer summary, there's an excellent
write up &lt;a href="http://blog.mrtz.org/2014/12/15/the-nips-experiment.html"&gt;here&lt;/a&gt;.  The
key message is this: &lt;strong&gt;"about 57% of the papers accepted by the
first committee were rejected by the second one and vice versa. In other words,
most papers at NIPS would be rejected if one reran the conference review
process (with a 95% confidence interval of 40-75%)."&lt;/strong&gt;  Publishing a paper at NIPS
can completely change the arc of your career - so the idea that someone's future
career is at the whims of chance is fairly discouraging.&lt;/p&gt;
&lt;p&gt;Although the results were bleak, they were not surprising.  Scientists
tend to be a fairly rational bunch &lt;sup&gt;[citation_needed]&lt;/sup&gt; but when it
comes to getting papers published and grants accepted, they fall back on superstition
and paranoia.&lt;/p&gt;
&lt;p&gt;&lt;a href="www.smbc.com"&gt;&lt;img alt="img" src="http://www.smbc-comics.com/comics/20120324.gif" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I know this first hand: with collaborators, we currently have two papers under review in
fairly selective journals, and we spent months worth of work in trying
 to parry possible comments from reviewers/editors.  For example, we had
 conversations like: "If we get X to review our paper, we need to make sure
 that Y is done because that's their theory and they will instantly reject
the paper if we don't discuss it".  Or "If our reviewer is a person who doesn't
like mathematical modeling then we need to show the simpler simulations
in the main text and the more complex ones in the supplementary."  A truly
embarrassing amount of work is done not because the authors think it makes the
paper stronger, but because scientists are utterly paranoid about what will
happen during the peer review process.&lt;/p&gt;
&lt;p&gt;There are two main reasons for this paranoia.  The first one is the
disproportionate influence played by grants obtained/impact factor when evaluating
scientists.  The current system really sucks, but this is an area where only
people with actual power to change the incentive structure (professors on tenure
committees, leaders of funding bodies, etc) can make a difference.  The second
reason is that &lt;strong&gt;the peer review process is remarkably un-transparent and in the
absence of real information people fall back on anecdotes and superstition&lt;/strong&gt;.  I
thought that this was an area where I could actually give a small contribution.&lt;/p&gt;
&lt;p&gt;With that in mind, after NIPS published their experiments, I reached out to the
eLife editorial team (I had an excellent experience publishing a paper there
previously, and they have a strong commitment to improving publishing) about
doing a serious statistical analysis of their peer review process.  Their team
(Mark Peterson, Peter Rodgers, and especially Andy Collins) was incredibly
pleasant to work with, and really cared about improving scientific publishing.
Unfortunately, we were unable to actually do randomized controlled experiments
the way NIPS did.  eLife is growing very quickly (data in the paper!) and
splitting up papers with dual review tracks would have been too taxing for
their resources at this point.&lt;/p&gt;
&lt;p&gt;What we ended up doing instead was looking at what factors influence how quickly
a paper gets published, and what influences how often a paper gets cited.  More
importantly, we make a &lt;a href="https://github.com/FedericoV/eLife_Editorial_Process/tree/master/data"&gt;very interesting dataset&lt;/a&gt;
available for everyone to analyze however they want, and we have the
&lt;a href="https://github.com/FedericoV/eLife_Editorial_Process"&gt;code&lt;/a&gt; to reproduce almost
every step of our analysis.&lt;/p&gt;
&lt;h2&gt;Further Steps:&lt;/h2&gt;
&lt;p&gt;The data I published is a snapshot of the situation of eLife as of early 2016.
Since all the scripts I used are online, it will be interesting to monitor the
situation and see how things change in the future.  Further, F1000 makes it very
easy to add new versions of a paper, so I am planning on trying to extend the
work and examine other effects, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;http://www.nature.com/news/papers-with-shorter-titles-get-more-citations-1.18246&lt;/li&gt;
&lt;li&gt;http://www.nature.com/news/rejection-improves-eventual-impact-of-manuscripts-1.11583&lt;/li&gt;
&lt;li&gt;http://link.springer.com/article/10.1007/s11192-016-1936-z&lt;/li&gt;
&lt;/ul&gt;</summary><category term="scientific publishing"></category><category term="open science"></category></entry><entry><title>Yet Another Instance of a Broken Publishing System at Work</title><link href="http://federicov.github.io/killing-animals-for-nothing.html" rel="alternate"></link><published>2015-10-15T00:00:00+02:00</published><updated>2015-10-15T00:00:00+02:00</updated><author><name>Federico Vaggi</name></author><id>tag:federicov.github.io,2015-10-15:killing-animals-for-nothing.html</id><summary type="html">&lt;h2&gt;Reproducibility in the Spotlight:&lt;/h2&gt;
&lt;p&gt;Psychology is still reeling from the results of a &lt;a href="https://www.sciencemag.org/content/349/6251/aac4716.abstract"&gt;massive study of
reproducibility&lt;/a&gt;
which found that less than half of statistically significant findings recently
published in the top psychology journals remain significant when independently
reproduced.  If this is new to you, Jesse Singal wrote a great summary about it &lt;a href="http://nymag.com/scienceofus/2015/08/many-psychology-research-findings-may-be-false.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Biology is not exactly doing that much better.  Ioannidis first highlighted
shoddy research practices in biology with his aptly named paper
&lt;a href="http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124"&gt;Why Most Published Research Findings Are False&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The latest entry in this saga is discussed in &lt;a href="http://www.nature.com/news/poorly-designed-animal-experiments-in-the-spotlight-1.18559"&gt;this&lt;/a&gt;
excellent article from Nature.  In papers that rely on animal models, the
results are too fragile and unreliable to be used as a basis for further
research because the statistical design of the experiments is often
critically flawed.  This is unfortunately not a huge surprise: scientists
at Bayer had already warned that published data had become &lt;a href="http://www.nature.com/nrd/journal/v10/n9/full/nrd3439-c1.html"&gt;progressively less reliable&lt;/a&gt; as
basis for drug research.&lt;/p&gt;
&lt;p&gt;In &lt;a href="http://www.nature.com/news/2011/110928/full/477511a.html"&gt;a previous editorial&lt;/a&gt;,
Malcolm Macleod points out that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The most reliable animal studies are those that: use randomization to eliminate
systematic differences between treatment groups; induce the condition under
investigation without knowledge of whether or not the animal will get the drug of
interest; and assess the outcome in a blinded fashion. Studies that do not
report these measures are much more likely to overstate the efficacy of
interventions.  &lt;strong&gt;Unfortunately, at best one in three publications follows
these basic protections against bias&lt;/strong&gt;. This suggests that authors,
reviewers and editors accord them little importance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For those that aren't familiar with experimental design - these aren't hyper
advanced techniques.  Those are all things which are taught in a 1st year graduate
experimental design course and that every senior scientist should be very
familiar with.&lt;/p&gt;
&lt;h2&gt;Animal Models In Science:&lt;/h2&gt;
&lt;p&gt;What makes this particularly frustrating is that in this case, the scientists
aren't just wasting public money, but they are killing loads of animals for
absolutely no public good.  Until computational models massively improve,
research with animal models is absolutely necessary, especially in the later
stages, when we wish to validate promising drug targets or test drug safety.&lt;/p&gt;
&lt;p&gt;In theory, the use of animal models in research in the US is
&lt;a href="https://grants.nih.gov/grants/olaw/references/phspol.htm"&gt;strictly regulated&lt;/a&gt;
by the NIH.  In practice, while charges of animal cruelty are taken seriously,
poor protocol design leading to a waste of model animals is very rarely
sanctioned.&lt;/p&gt;
&lt;h2&gt;You Always Get What You Measure:&lt;/h2&gt;
&lt;p&gt;Why do really smart scientists make such stupid mistakes?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Arrogance&lt;/em&gt;: &lt;a href="http://retractionwatch.com/2014/05/30/braggadacio-information-control-and-fear-life-inside-a-brigham-stem-cell-lab-under-investigation/"&gt;Some oldschool PIs&lt;/a&gt; are so confident in their own theories that they see
statistics not as a critical way to evaluate their data, but as a simple
threshold (p &amp;lt; 0.05) they gotta cross to be able to publish.  This is thankfully
not very common, but if you go to any bioinformatics conference and talk over drinks
to some of the junior people there, you'll hear all sorts of horror stories about
being the only bioinformatician in a group and having to somehow come up with a
statistical test that gives p&amp;lt;0.05.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Misaligned Incentives&lt;/em&gt;: This is unfortunately very common.  The currency of
scientific careers are publications in high impact journals, and the easiest way
to publish in those journals is to produce splashy results.  Unfortunately,
results are the one thing that scientists have no actual control over - if you
execute a well designed experiment to test a reasonable hypothesis, the
actual outcome is up to the Universe.  By dis-proportionally rewarding splashy
results, we punish the scientists that actually do research properly, since they
will have a much lower fraction of positive results.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The irony that Nature, one of the most prestigious scientific journals in the world,
acknowledges that scientists push splashy but unreliable findings (like say,
&lt;a href="http://www.nature.com/nature/journal/v505/n7485/full/nature12968.html"&gt;these&lt;/a&gt;)
instead of solid but boring ones is not lost on me.  It reminds me of the time
a Facebook executive complained &lt;a href="https://www.facebook.com/mhudack/posts/10152148792566194"&gt;that the Internet was drowning in shitty click-baity articles&lt;/a&gt;, when the
product he was responsible for was the main driver of that behaviour.&lt;/p&gt;
&lt;p&gt;It's important to add that the quest for high impact publications is not
just motivated by a selfish desire for success.  Postdocs are facing a very
difficult job market and are desperate to have a high impact paper before
applying for tenure track positions.  PIs are under an &lt;a href="http://www.dcscience.net/2014/12/01/publish-and-perish-at-imperial-college-london-the-death-of-stefan-grimm/"&gt;immense amount of
pressure&lt;/a&gt;
by their institutions to obtain grants, and it's getting harder and harder.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://nexus.od.nih.gov/all/wp-content/uploads/2014/03/FundingAwardSuccessRate_RPG.jpg " alt="NIH_Funding" style="width: 700px;"/&gt;&lt;/p&gt;
&lt;p&gt;Further, missing grants doesn't just hurt the career of the PI.  Very few people
in science have the luxury of fixed positions - even lab technicians and staff
scientists are often funded by grants, and not getting a grant renewed can mean
having to fire people people you've worked with for many years.&lt;/p&gt;</summary><category term="scientific publishing"></category><category term="incentives"></category><category term="research"></category></entry><entry><title>Why Scientists Don't Share Data and How to Fix it</title><link href="http://federicov.github.io/novelty-is-overrated.html" rel="alternate"></link><published>2014-10-29T00:00:00+01:00</published><updated>2014-10-29T00:00:00+01:00</updated><author><name>Federico Vaggi</name></author><id>tag:federicov.github.io,2014-10-29:novelty-is-overrated.html</id><summary type="html">&lt;h2&gt;Towards a Better Science&lt;/h2&gt;
&lt;p&gt;Recently, there's been a highly encouraging push towards a more open science with a greater emphasis on &lt;a href="http://www.nature.com/nature/focus/reproducibility/"&gt;reproducibility&lt;/a&gt;, spurred by a few &lt;a href="http://retractionwatch.com/2014/09/11/potentially-groundbreaking-highly-provocative-nature-stap-cell-peer-reviews-published/"&gt;high profile retractions&lt;/a&gt; and a growing awareness of the &lt;a href="http://www.economist.com/news/briefing/21588057-scientists-think-science-self-correcting-alarming-degree-it-not-trouble"&gt;slippery nature&lt;/a&gt; of several scientific findings.&lt;/p&gt;
&lt;p&gt;This slow shift has important implications.  While in theory journals encourage or require the sharing of published data to anyone who requests it, often this is very difficult or even unpleasant, requiring multiple requests to journal editors to compel the authors to share the raw data.  Why is this data sharing so controversial and complex?&lt;/p&gt;
&lt;h2&gt;Incentives and Data Sharing&lt;/h2&gt;
&lt;p&gt;Andrew Gellman wrote about this from the point of view of political science and statistics both on his blog&lt;a href="http://andrewgelman.com/2012/08/07/whats-stoppin-people-from-sharin-data-and-code/"&gt;1&lt;/a&gt; &lt;a href="http://themonkeycage.org/2012/08/03/things-that-arent-prisoners-dilemmas-part-2/"&gt;2&lt;/a&gt; and &lt;a href="http://www.stat.columbia.edu/~gelman/research/published/ChanceEthics1.pdf"&gt;here&lt;/a&gt;.  As he discusses - part of the problem is technical, and part of the problem is a matter of misaligned incentives.&lt;/p&gt;
&lt;p&gt;The technical part of the problem is that sharing big data sets is very complex, and, until recently, getting financed to work on infrastructure was very difficult.  Time spent figuring out how to put a big data set online in a way that could be producitively mined has potentially very little return.  Thankfully, there's some effort on this front - with &lt;a href="http://ivory.idyll.org/blog/2014-moore-ddd-stmt-of-work.html"&gt;Titus Brown&lt;/a&gt; and the &lt;a href="http://www.moore.org/programs/science/data-driven-discovery"&gt;Moore Foundation&lt;/a&gt; stepping in to hopefully make this easier.&lt;/p&gt;
&lt;p&gt;The other obstacle in the way of greater data sharing is a matter of incentives.  Putting raw data in a nice format, and annotating it properly takes a lot of time, that leads to very little tangible rewards.  PIs often build careers from the results of a long running study, and sharing all the data can put their competitive advantage at risk.  In biology - there's also an additional element at work, as many journals (the big three especially) overwhelmingly favour papers with new experiments over papers that obtain novel results from existing datasets.&lt;/p&gt;
&lt;p&gt;This bias towards new experimental results is highly counter productive, and puts scientists in a terrible spot.  In the constant quest to maximize impact (a necessity in the current funding climate) scientists have to decide how to best spread out the results from a big study among the biggest possible number of papers, making sure that no paper clips the novelty of any of the other papers.  For an example of this, look at the collection of papers from the &lt;a href="http://www.genome.gov/encode/"&gt;Encode project&lt;/a&gt;, where an immense amount of planning went into figuring out how to maximize the number of first author papers from the study.&lt;/p&gt;
&lt;h2&gt;What Makes a Paper New?&lt;/h2&gt;
&lt;p&gt;This emphasis on associating new experiments with novelty is largely a hold over from a previous era - where thinking of the critical experiment and performing it were far more difficult than analyzing the resulting data.  With the new data sets trickling in from high throughput experiments, obtaining insight from data is often more challenging than obtaining it in the first place, and the publishing guidelines should adjust to this new reality.&lt;/p&gt;
&lt;p&gt;Although this is a very complex topic, I'd like to offer a few simple recommendations that don't require seismic shifts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Judge papers by how novel and robust their insights are, not how novel the data is.&lt;/li&gt;
&lt;li&gt;Shift away from the idea of 'first author' and 'last author'.  Part of what drives the need to produce multiple papers is that on large multi-year projects, you need to produce enough first author papers for all the postdocs and PhDs.  Especially on papers that require complex experiments + computational analysis, multiple people often contribute equally.&lt;/li&gt;
&lt;li&gt;Finally, develop a better mechanism to reward people who make highly sophisticated datasets available online.  Something in between a citation and co-authorship.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While I was putting the finishing touches on this blog, I was linked &lt;a href="http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.1001747"&gt;this&lt;/a&gt; excellent article by &lt;a href="https://med.stanford.edu/profiles/john-ioannidis"&gt;Ioannidis&lt;/a&gt; on how to make more published research true.  It's excellent, and everyone should read it, especially the part about the reward system.&lt;/p&gt;</summary><category term="scientific publishing"></category><category term="incentives"></category></entry></feed>